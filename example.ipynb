{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503817f8",
   "metadata": {
    "id": "503817f8"
   },
   "source": [
    "# RNAM: Recurrent Neural Additive Model\n",
    "\n",
    "A first-order [generalized additive model (GAM)](https://hastie.su.domains/Papers/gam.pdf) operating on tabular data $\\mathbf{x} \\in \\mathbb{R}^p$ can be written as\n",
    "\n",
    "$$\n",
    "\\Gamma(\\mathbf{x}) = b + \\sum_{i=1}^p f_i(x_i),\n",
    "$$\n",
    "\n",
    "and is interpretable under certain conditions.\n",
    "Constraints such as those described in [Hastie's and Tibshirani's seminal paper](https://hastie.su.domains/Papers/gam.pdf) or [Hooker's generalized functional ANOVA](https://www.jstor.org/stable/27594267) are not applied within this notebook.\n",
    "The core focus is an efficient estimation of unconstrained shape functions $f_i$ from longitudinal data using neural networks.\n",
    "Data used within this example are generated from uniform latents, a [Friedman-like function](https://www.slac.stanford.edu/pubs/slacpubs/2250/slac-pub-2336.pdf), as well as a wide and shallow [rnam.minGRU](rnam/gru.py).\n",
    "\n",
    "Additional related work includes:\n",
    "- [Berhane's and Tibshirani's GAMs for longitudinal data](https://doi.org/10.2307/3315715).\n",
    "- [Agarwal et al.'s Neural Additive Model](https://arxiv.org/pdf/2004.13912v2).\n",
    "\n",
    "Last accessed: 2025-12-15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa48ed2",
   "metadata": {
    "id": "dfa48ed2"
   },
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e4b8b",
   "metadata": {
    "id": "de3e4b8b"
   },
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a35d3e",
   "metadata": {
    "id": "c6a35d3e"
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 8\n",
    "BLOCKS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a6ff8",
   "metadata": {
    "id": "aa5a6ff8"
   },
   "source": [
    "Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ab6939",
   "metadata": {
    "id": "d8ab6939"
   },
   "outputs": [],
   "source": [
    "TERMS = 8\n",
    "WARMUP = 8\n",
    "SEQ_LEN = 1_024\n",
    "LATENT_DECAY = 0.95\n",
    "GENERATOR_BATCH_SIZE = 100\n",
    "SAMPLES = 10_000\n",
    "SPLIT_POINT = 8_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6719174",
   "metadata": {
    "id": "c6719174"
   },
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3f0e2b",
   "metadata": {
    "id": "3a3f0e2b"
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4c0eb",
   "metadata": {
    "id": "a1a4c0eb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c41e3e",
   "metadata": {
    "id": "02c41e3e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import rnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f032f718",
   "metadata": {
    "id": "f032f718"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7468514d",
   "metadata": {
    "id": "7468514d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75756d58",
   "metadata": {
    "id": "75756d58"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0bc0e",
   "metadata": {
    "id": "19f0bc0e"
   },
   "source": [
    "Additive layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d5be8d",
   "metadata": {
    "id": "12d5be8d"
   },
   "outputs": [],
   "source": [
    "class AdditiveLayer(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.bias + input.sum(0), input.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca284409",
   "metadata": {
    "id": "ca284409"
   },
   "source": [
    "Simple pre-norm residual block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f9b415",
   "metadata": {
    "id": "a5f9b415"
   },
   "outputs": [],
   "source": [
    "class RNAMBlock(nn.Module):\n",
    "    def __init__(self, terms: int, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.gru = rnam.minGRU(terms, dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            rnam.Linear(terms, dim, dim),\n",
    "            nn.ReLU(),\n",
    "            rnam.Linear(terms, dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return input + self.ff(input + self.gru(input)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0de381",
   "metadata": {
    "id": "fd0de381"
   },
   "source": [
    "First-order additive sequence regressor/classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e3ef5e",
   "metadata": {
    "id": "49e3ef5e"
   },
   "outputs": [],
   "source": [
    "class FirstOrderRNAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        terms: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        blocks: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.emb = rnam.Linear(terms=terms, in_dim=1, out_dim=hidden_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            RNAMBlock(terms, hidden_dim) for _ in range(blocks)\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            rnam.Linear(terms=terms, in_dim=hidden_dim, out_dim=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            rnam.Linear(terms=terms, in_dim=hidden_dim, out_dim=out_dim),\n",
    "        )\n",
    "        self.gam = AdditiveLayer(dim=out_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        input = self.emb(input)\n",
    "        for block in self.blocks:\n",
    "            input = block(input)\n",
    "        input = self.proj(input[:, :, -1])\n",
    "        return self.gam(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "yCC5BmQZyBvM",
   "metadata": {
    "id": "yCC5BmQZyBvM"
   },
   "outputs": [],
   "source": [
    "model = FirstOrderRNAM(\n",
    "    terms=TERMS,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_dim=1,\n",
    "    blocks=BLOCKS,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d48b0",
   "metadata": {
    "id": "1f0d48b0"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5599d24",
   "metadata": {
    "id": "e5599d24"
   },
   "outputs": [],
   "source": [
    "assert 4 <= TERMS\n",
    "assert SAMPLES % GENERATOR_BATCH_SIZE == 0\n",
    "assert SPLIT_POINT < SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f63b4b",
   "metadata": {
    "id": "a7f63b4b"
   },
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: list[tuple[torch.Tensor, torch.Tensor]],\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    features, targets = [], []\n",
    "    for x, y in batch:\n",
    "        features.append(x)\n",
    "        targets.append(y)\n",
    "\n",
    "    return torch.stack(features, dim=1), torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff814eb",
   "metadata": {
    "id": "8ff814eb"
   },
   "outputs": [],
   "source": [
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, features: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.targets.size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[:, idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24e97b",
   "metadata": {
    "id": "3b24e97b"
   },
   "source": [
    "The EMA over time of latents $\\mathbf{z} \\sim \\text{Uniform}(0, 1)$ is transformed into the target defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796dd66d",
   "metadata": {
    "id": "796dd66d"
   },
   "outputs": [],
   "source": [
    "def friedman(z: torch.Tensor) -> torch.Tensor:\n",
    "    ema = torch.zeros_like(z[:, :, 0])\n",
    "    for t in range(z.size(-2)):\n",
    "        ema = LATENT_DECAY * ema + (1 - LATENT_DECAY) * z[:, :, t]\n",
    "    return ema[0].sin() + ema[1].cos() + ema[2].abs().sqrt() + ema[3].square()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2966b",
   "metadata": {
    "id": "4cf2966b"
   },
   "source": [
    "Inputs $\\mathbf{X}$ are generated from $\\text{Generator}(\\mathbf{Z})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71aa9dc1",
   "metadata": {
    "id": "71aa9dc1"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        dim = HIDDEN_DIM * BLOCKS\n",
    "        self.emb = rnam.Linear(terms=TERMS, in_dim=1, out_dim=dim)\n",
    "        self.gru = rnam.minGRU(terms=TERMS, dim=dim)\n",
    "        self.proj = rnam.Linear(terms=TERMS, in_dim=dim, out_dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self, input: torch.Tensor, prev_hidden: torch.Tensor | None = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        input = self.emb(input)\n",
    "        input, prev_hidden = self.gru(input, prev_hidden)\n",
    "        return self.proj(input), prev_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b459ec",
   "metadata": {
    "id": "d8b459ec"
   },
   "source": [
    "Uniformly distributed warmup noise is used to stabilize the recurrent generator.\n",
    "Both inputs and targets are z-score normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1f97dd",
   "metadata": {
    "id": "9d1f97dd"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_data() -> tuple[Dataset, Dataset]:\n",
    "    generator = Generator()\n",
    "    generator.eval()\n",
    "\n",
    "    features, targets = [], []\n",
    "    for _ in range(SAMPLES // GENERATOR_BATCH_SIZE):\n",
    "        z = torch.rand(TERMS, GENERATOR_BATCH_SIZE, WARMUP + SEQ_LEN, 1)\n",
    "        targets.append(friedman(z))\n",
    "\n",
    "        batch, prev_hidden = [], None\n",
    "        for t in range(WARMUP + SEQ_LEN):\n",
    "            hidden, prev_hidden = generator(z[:, :, t : t + 1], prev_hidden)\n",
    "            if WARMUP < t + 1:\n",
    "                batch.append(hidden.detach())\n",
    "\n",
    "        features.append(torch.cat(batch, dim=-2))\n",
    "\n",
    "    features = torch.cat(features, dim=1)\n",
    "    mu = torch.mean(features, (-3, -2, -1), keepdim=True)\n",
    "    sigma = torch.std(features, (-3, -2, -1), correction=0, keepdim=True)\n",
    "    features = (features - mu) / sigma\n",
    "\n",
    "    targets = torch.cat(targets)\n",
    "    targets = (targets - targets.mean()) / targets.std()\n",
    "\n",
    "    train_data = DataWrapper(features[:, :SPLIT_POINT], targets[:SPLIT_POINT])\n",
    "    val_data = DataWrapper(features[:, SPLIT_POINT:], targets[SPLIT_POINT:])\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d58cd58",
   "metadata": {
    "id": "3d58cd58"
   },
   "outputs": [],
   "source": [
    "train_data, val_data = generate_data()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ddbf",
   "metadata": {
    "id": "40a4ddbf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3pM3SDfDxvub",
   "metadata": {
    "id": "3pM3SDfDxvub"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdcc0e",
   "metadata": {},
   "source": [
    "Executed on the Google Colab free tier (T4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7c6865",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c7c6865",
    "outputId": "a91ec943-00b2-429e-eb9a-51b9749f741b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001: 1.274356/0.934273\n",
      "002: 0.817840/0.823477\n",
      "003: 0.728004/0.733901\n",
      "004: 0.625222/0.639761\n",
      "005: 0.510634/0.497099\n",
      "006: 0.383550/0.314465\n",
      "007: 0.241109/0.198362\n",
      "008: 0.156576/0.124920\n",
      "009: 0.112902/0.102335\n",
      "010: 0.101714/0.100229\n",
      "011: 0.100948/0.095167\n",
      "012: 0.098225/0.095685\n",
      "013: 0.094544/0.086277\n",
      "014: 0.089156/0.107411\n",
      "015: 0.087195/0.116670\n",
      "016: 0.087054/0.082870\n",
      "017: 0.087589/0.084297\n",
      "018: 0.089546/0.090741\n",
      "019: 0.089298/0.081064\n",
      "020: 0.083044/0.080673\n",
      "021: 0.083245/0.079981\n",
      "022: 0.078800/0.079017\n",
      "023: 0.080868/0.078642\n",
      "024: 0.078069/0.079202\n",
      "025: 0.080394/0.077141\n",
      "total time: 495.89863707200004\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        y_hat, _ = model(x)\n",
    "\n",
    "        loss = criterion(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            y_hat, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"{epoch + 1:03d}: {train_loss:.6f}/{val_loss:.6f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "print(f\"total time: {time.perf_counter() - start}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
